{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"mount_file_id":"1aDiop5c-V90dbJAUkJjgfpwZHa4vvdOa","authorship_tag":"ABX9TyPBXNhzemTUvVqvaZ1aOpnF"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"e7q1DhWpV-Dw"},"source":["# Auto-diff with JAX\n","\n","https://github.com/google/jax"]},{"cell_type":"code","metadata":{"id":"Si_DpGyVWZ5I"},"source":["from matplotlib.image import imread\n","import matplotlib.pyplot as plt\n","plt.rcParams['figure.figsize'] = [8, 8]\n","A = imread('drive/My Drive/polimi/NAML/LAB/data/JAX.png')\n","img = plt.imshow(A)\n","plt.axis('off')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Snjf4u7vf0sa"},"source":["JAX is a Google research project, developed by the former developers of [Autograd](https://github.com/hips/autograd), bringing together the potentialities of Autograd and the linear algebra accelerator [XLA](https://www.tensorflow.org/xla). It is based on three pillars:\n","- `grad`: Automatic Differentiation\n","- `jit`: Just-in-time compilation\n","- `vmap`: Automatic vectorization.\n","\n","## Automatic differentiation in JAX\n","\n","JAX augments numpy and Python code with function transformations which make it trivial to perform operations common in machine learning programs. JAX's augmented numpy lives at `jax.numpy`. With a few exceptions, you can think of `jax.numpy` as directly interchangeable with `numpy`. As a general rule, you should use `jax.numpy` whenever you plan to use any of JAX's transformations.\n","\n","The function `df = jax.grad(f, argnums = 0)` takes the callable object `f` and returns another callable object, `df`, evaluating the gradient of `f` w.r.t. the argument(s) of index(es) `argnums`. For more information, check out the [documentation](https://jax.readthedocs.io/en/latest/jax.html?highlight=grad#jax.grad)."]},{"cell_type":"markdown","metadata":{"id":"yBVAFA6LiZhv"},"source":["**Example**\n","\n","We consider the function:\n","$$\n","f(x) = x \\sin(x^2)\n","$$\n","\n","and we compute $f'(x_0)$ for $x_0 = 0.13$"]},{"cell_type":"code","metadata":{"id":"pJsIHEuC0BwB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668116008599,"user_tz":-60,"elapsed":1748,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}},"outputId":"1d82508a-2f51-48ee-96cd-66cf85dce20e"},"source":["import numpy as np\n","import jax.numpy as jnp\n","import jax\n","\n","func = lambda x : x*jnp.sin(x**2)\n","x0 = 0.13\n","dfunc_AD = jax.grad(func)\n","df_AD = dfunc_AD(x0)\n","\n","# analytical derivative\n","dfunc = lambda x : np.sin(x**2)+2 * x**2 * np.cos(x**2)\n","df_ex = dfunc(x0)\n","\n","print('df (ex): %f' % df_ex)\n","print('df (AD): %f' % df_AD)\n","\n","print('err (AD): %e' % (abs(df_AD - df_ex)/abs(df_ex)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"]},{"output_type":"stream","name":"stdout","text":["df (ex): 0.050694\n","df (AD): 0.050694\n","err (AD): 7.348529e-08\n"]}]},{"cell_type":"markdown","source":["Evaluate the execution times of the functions `func` and `dfunc_AD`."],"metadata":{"id":"APS0GGb4fe-7"}},{"cell_type":"code","source":["%timeit func(x0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p17OzfdQfNM0","executionInfo":{"status":"ok","timestamp":1668116448258,"user_tz":-60,"elapsed":7261,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}},"outputId":"c433511c-46ae-42bb-cad2-7c1e2a7bb332"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8.59 µs ± 211 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"]}]},{"cell_type":"code","source":["%timeit dfunc_AD(x0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YtnZaJk3dyDF","executionInfo":{"status":"ok","timestamp":1668116460366,"user_tz":-60,"elapsed":3752,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}},"outputId":"24f9876c-0df7-4d94-c29b-4e9ecfe42e4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.35 ms ± 141 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"]}]},{"cell_type":"markdown","metadata":{"id":"v9ziWf_lcjLn"},"source":["### Speed it up with JIT!\n","\n","Compile the functions `func` and `dfunc_AD` using the [just-in-time compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation) utility `jax.jit`. \n","\n","With `f_jit = jax.jit(f)` a callable `f` is compiled into `f_jit`.\n","\n","Then, check that the compiled functions return the same results as the original ones. Finally, evaluate the execution times and compare it with the previous results."]},{"cell_type":"code","source":["func_jit = jax.jit(func)\n","f_jit = func_jit(x0)\n","func(x0) - f_jit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_oW5fnXne7r9","executionInfo":{"status":"ok","timestamp":1668116398535,"user_tz":-60,"elapsed":197,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}},"outputId":"34cfc924-fef5-45d1-d680-de789f0890d4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DeviceArray(0., dtype=float32, weak_type=True)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["%timeit func_jit(x0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iFeRZbOUfQs9","executionInfo":{"status":"ok","timestamp":1668116467025,"user_tz":-60,"elapsed":2890,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}},"outputId":"0c79ed27-f4b4-4785-b32a-437faf1a4755"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.27 µs ± 95.5 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"]}]},{"cell_type":"code","source":["dfunc_AD_jit = jax.jit(dfunc_AD)\n","df_AD_jit = dfunc_AD_jit(x0)\n","df_AD - df_AD_jit"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nyn03naKeiG6","executionInfo":{"status":"ok","timestamp":1668116307102,"user_tz":-60,"elapsed":336,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}},"outputId":"12e3af00-2ab0-4454-cccb-ac5406485b04"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DeviceArray(0., dtype=float32, weak_type=True)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["%timeit dfunc_AD_jit(x0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLxmzBtheuqX","executionInfo":{"status":"ok","timestamp":1668116473482,"user_tz":-60,"elapsed":2994,"user":{"displayName":"Francesco Regazzoni","userId":"08593163129562527691"}},"outputId":"d64f8a37-5f6e-4f43-dbaf-402938a6e3a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.56 µs ± 118 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"]}]}]}