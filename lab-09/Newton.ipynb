{"cells":[{"cell_type":"markdown","metadata":{"id":"bZCDNUgw0nI3"},"source":["# Newton method"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"IJcJCwFhc8cs"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import jax.numpy as jnp\n","import jax\n","\n","# We enable double precision in JAX\n","from jax.config import config\n","config.update(\"jax_enable_x64\", True)"]},{"cell_type":"markdown","metadata":{"id":"7qkdJdGg1AP1"},"source":["We consider a random matrix $A \\in \\mathbb{R}^{n\\times n}$, with $n = 100$ and a random vector $\\mathbf{x}_{\\text{ex}} \\in \\mathbb{R}^n$.\n","We define then $\\mathbf{b} = A \\, \\mathbf{x}_{\\text{ex}}$."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"c0h8ihCddDPf"},"outputs":[],"source":["n = 100\n","\n","np.random.seed(0)\n","A = np.random.randn(n,n)\n","x_ex = np.random.randn(n)\n","b = A @ x_ex"]},{"cell_type":"markdown","metadata":{"id":"UanVhF4xAVoX"},"source":["Define the loss function\n","\n","$$\n","\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_2^2\n","$$"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["Array(0., dtype=float64)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["def loss(x):\n","    return jnp.sum(jnp.square(A @ x - b))\n","\n","loss(x_ex)"]},{"cell_type":"markdown","metadata":{"id":"uAZ9XGaiAs3X"},"source":["By using the `jax` library, implement and compile functins returning the gradient ($\\nabla \\mathcal{J}(\\mathbf{x})$) and the hessian ($\\nabla^2 \\mathcal{J}(\\mathbf{x})$) of the loss function (*Hint*: use the `jacrev` or the `jacfwd`) function."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"KflmuLXld2T4"},"outputs":[],"source":["grad = jax.grad(loss)\n","hess = jax.jacfwd(jax.jacrev(loss))\n","\n","loss_jit = jax.jit(loss)\n","grad_jit = jax.jit(grad)\n","hess_jit = jax.jit(hess)"]},{"cell_type":"markdown","metadata":{"id":"bSMg8ocDBndO"},"source":["Check that the results are correct (up to machine precision)."]},{"cell_type":"code","execution_count":30,"metadata":{"id":"xZulGRQ1efFP"},"outputs":[{"name":"stdout","output_type":"stream","text":["2.1550089998180016e-12\n","4.829664679334261e-13\n"]}],"source":["np.random.seed(0)\n","x_guess = np.random.randn(n)\n","\n","G_ad = grad_jit(x_guess)\n","G_ex = 2 * A.T @ (A @ x_guess - b)\n","print(np.linalg.norm(G_ad - G_ex))\n","\n","H_ad = hess_jit(x_guess)\n","H_ex = 2 * A.T @ A\n","print(np.linalg.norm(H_ad - H_ex))"]},{"cell_type":"markdown","metadata":{"id":"b-gA_kKPB2SV"},"source":["Exploit the formula\n","$$\n","\\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v} = \\nabla_{\\mathbf{x}} \\phi(\\mathbf{x}, \\mathbf{v})\n","$$\n","where \n","$$\n","\\phi(\\mathbf{x}, \\mathbf{v}) := \\nabla \\mathcal{J}(\\mathbf{x}) \\cdot \\mathbf{v}\n","$$\n","to write an optimized function returning the hessian-vector-product\n","$$\n","(\\mathbf{x}, \\mathbf{v}) \\mapsto \\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v}.\n","$$\n","Compare the computational performance w.r.t. the full hessian computation."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1638461194311,"user":{"displayName":"Francesco Regazzoni","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08593163129562527691"},"user_tz":-60},"id":"T9969dU4kc6f","outputId":"368b2173-e971-474c-e3b1-f649d9bb15d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["1.2744887647117243e-12\n"]}],"source":["np.random.seed(1)\n","v = np.random.randn(n)\n","\n","hvp_basic = lambda x, v: hess(x) @ v\n","gvp = lambda x, v: jax.jvp(loss, [x], [v])[1] #jax.dot(grad(x),v)\n","hvp = lambda x, v: jax.grad(gvp, argnums=0)(x,v)\n","\n","hvp_basic_jit = jax.jit(hvp_basic)\n","hvp_jit = jax.jit(hvp)\n","\n","Hv_ad = hvp_jit(x_guess, v)\n","Hv_ex = H_ex @ v\n","print(np.linalg.norm(Hv_ad - Hv_ex))"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"jsA4eUnuj3ju"},"outputs":[{"name":"stdout","output_type":"stream","text":["224 µs ± 4.9 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n","16.4 µs ± 54.8 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"]}],"source":["%timeit hvp_basic_jit(x_guess, v)\n","%timeit hvp_jit(x_guess, v)"]},{"cell_type":"markdown","metadata":{"id":"TagmrdjG4Ww4"},"source":["Implement the Newton method for the minimization of the loss function $\\mathcal{L}$. Set a maximim number of 100 iterations and a tolerance on the increment norm of $\\epsilon = 10^{-8}$."]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["======Epoch 0========\n","Loss = 3.738843e+04\n","Increment = 1.547522e+01\n","======Epoch 1========\n","Loss = 1.434584e-22\n","Increment = 1.252722e-09\n"]}],"source":["x = x_guess.copy()\n","n_epochs = 100\n","tolerance = 1e-8\n","\n","for epoch in range(n_epochs):\n","    loss_val = loss_jit(x)\n","    grad_val = grad_jit(x)\n","    hess_val = hess_jit(x)\n","    \n","    increment = jnp.linalg.solve(hess_val, -grad_val)\n","    \n","    x += increment\n","    norm_increment = jnp.linalg.norm(increment)\n","    print(f'======Epoch {epoch}========')\n","    print(f'Loss = {loss_val:3e}')\n","    print(f'Increment = {norm_increment:3e}')\n","    \n","    if jnp.linalg.norm(increment) < tolerance:\n","        break\n","    "]},{"cell_type":"markdown","metadata":{"id":"uNL7303C4oTL"},"source":["Repeat the optimization loop for the loss function\n","\n","$$\n","\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_4^4\n","$$"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"data":{"text/plain":["Array(0., dtype=float64)"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["def loss_4(x):\n","    return jnp.sum(jnp.power(A @ x - b, 4))\n","\n","loss_4(x_ex)"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["grad_4 = jax.grad(loss_4)\n","hess_4 = jax.jacfwd(jax.jacrev(loss_4))\n","\n","loss_4_jit = jax.jit(loss_4)\n","grad_4_jit = jax.jit(grad_4)\n","hess_4_jit = jax.jit(hess_4)"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["======Epoch 0========\n","Loss = 2.432931e+08\n","Increment = 5.158408e+00\n","======Epoch 1========\n","Loss = 4.805790e+07\n","Increment = 3.438938e+00\n","======Epoch 2========\n","Loss = 9.492918e+06\n","Increment = 2.292626e+00\n","======Epoch 3========\n","Loss = 1.875144e+06\n","Increment = 1.528416e+00\n","======Epoch 4========\n","Loss = 3.703989e+05\n","Increment = 1.018945e+00\n","======Epoch 5========\n","Loss = 7.316521e+04\n","Increment = 6.792965e-01\n","======Epoch 6========\n","Loss = 1.445239e+04\n","Increment = 4.528642e-01\n","======Epoch 7========\n","Loss = 2.854793e+03\n","Increment = 3.019095e-01\n","======Epoch 8========\n","Loss = 5.639097e+02\n","Increment = 2.012729e-01\n","======Epoch 9========\n","Loss = 1.113896e+02\n","Increment = 1.341820e-01\n","======Epoch 10========\n","Loss = 2.200288e+01\n","Increment = 8.945465e-02\n","======Epoch 11========\n","Loss = 4.346247e+00\n","Increment = 5.963644e-02\n","======Epoch 12========\n","Loss = 8.585180e-01\n","Increment = 3.975763e-02\n","======Epoch 13========\n","Loss = 1.695838e-01\n","Increment = 2.650508e-02\n","======Epoch 14========\n","Loss = 3.349803e-02\n","Increment = 1.767006e-02\n","======Epoch 15========\n","Loss = 6.616896e-03\n","Increment = 1.178005e-02\n","======Epoch 16========\n","Loss = 1.307041e-03\n","Increment = 7.853358e-03\n","======Epoch 17========\n","Loss = 2.581810e-04\n","Increment = 5.235572e-03\n","======Epoch 18========\n","Loss = 5.099871e-05\n","Increment = 3.490383e-03\n","======Epoch 19========\n","Loss = 1.007382e-05\n","Increment = 2.326920e-03\n","======Epoch 20========\n","Loss = 1.989890e-06\n","Increment = 1.551280e-03\n","======Epoch 21========\n","Loss = 3.930647e-07\n","Increment = 1.034187e-03\n","======Epoch 22========\n","Loss = 7.764241e-08\n","Increment = 6.894579e-04\n","======Epoch 23========\n","Loss = 1.533677e-08\n","Increment = 4.596389e-04\n","======Epoch 24========\n","Loss = 3.029486e-09\n","Increment = 3.064257e-04\n","======Epoch 25========\n","Loss = 5.984170e-10\n","Increment = 2.042839e-04\n","======Epoch 26========\n","Loss = 1.182058e-10\n","Increment = 1.361893e-04\n","======Epoch 27========\n","Loss = 2.334930e-11\n","Increment = 9.079282e-05\n","======Epoch 28========\n","Loss = 4.612207e-12\n","Increment = 6.052853e-05\n","======Epoch 29========\n","Loss = 9.110533e-13\n","Increment = 4.035235e-05\n","======Epoch 30========\n","Loss = 1.799611e-13\n","Increment = 2.690156e-05\n","======Epoch 31========\n","Loss = 3.554788e-14\n","Increment = 1.793438e-05\n","======Epoch 32========\n","Loss = 7.021803e-15\n","Increment = 1.195626e-05\n","======Epoch 33========\n","Loss = 1.387023e-15\n","Increment = 7.970834e-06\n","======Epoch 34========\n","Loss = 2.739798e-16\n","Increment = 5.313894e-06\n","======Epoch 35========\n","Loss = 5.411947e-17\n","Increment = 3.542594e-06\n","======Epoch 36========\n","Loss = 1.069027e-17\n","Increment = 2.361730e-06\n","======Epoch 37========\n","Loss = 2.111658e-18\n","Increment = 1.574486e-06\n","======Epoch 38========\n","Loss = 4.171175e-19\n","Increment = 1.049657e-06\n","======Epoch 39========\n","Loss = 8.239359e-20\n","Increment = 6.997717e-07\n","======Epoch 40========\n","Loss = 1.627528e-20\n","Increment = 4.665147e-07\n","======Epoch 41========\n","Loss = 3.214869e-21\n","Increment = 3.110095e-07\n","======Epoch 42========\n","Loss = 6.350359e-22\n","Increment = 2.073397e-07\n","======Epoch 43========\n","Loss = 1.254392e-22\n","Increment = 1.382265e-07\n","======Epoch 44========\n","Loss = 2.477811e-23\n","Increment = 9.215105e-08\n","======Epoch 45========\n","Loss = 4.894442e-24\n","Increment = 6.143399e-08\n","======Epoch 46========\n","Loss = 9.668034e-25\n","Increment = 4.095599e-08\n","======Epoch 47========\n","Loss = 1.909735e-25\n","Increment = 2.730401e-08\n","======Epoch 48========\n","Loss = 3.772316e-26\n","Increment = 1.820266e-08\n","======Epoch 49========\n","Loss = 7.451489e-27\n","Increment = 1.213512e-08\n","======Epoch 50========\n","Loss = 1.471899e-27\n","Increment = 8.090076e-09\n"]}],"source":["x = x_guess.copy()\n","n_epochs = 100\n","tolerance = 1e-8\n","\n","for epoch in range(n_epochs):\n","    loss_val = loss_4_jit(x)\n","    grad_val = grad_4_jit(x)\n","    hess_val = hess_4_jit(x)\n","    \n","    increment = jnp.linalg.solve(hess_val, -grad_val)\n","    \n","    x += increment\n","    norm_increment = jnp.linalg.norm(increment)\n","    print(f'======Epoch {epoch}========')\n","    print(f'Loss = {loss_val:3e}')\n","    print(f'Increment = {norm_increment:3e}')\n","    \n","    if jnp.linalg.norm(increment) < tolerance:\n","        break\n","    "]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNWgVsFWCHZ4kicE3q6gQM4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
